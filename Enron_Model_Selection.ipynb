{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "    \n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import  MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "#Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest,chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Feature selection\n",
    "from sklearn.decomposition import PCA,RandomizedPCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest,chi2, f_regression\n",
    "\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the next steps the data will be cleaned, if you want to understand the process, check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_label = 'poi'                \n",
    "\n",
    "financial_features_list = [\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary'\n",
    "]\n",
    "features_list = [target_label] + financial_features_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"P:/Nanodegree/ML/ud120-projects/tools/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = [\"TOTAL\", \"THE TRAVEL AGENCY IN THE PARK\", \"LOCKHART EUGENE E\", \"CHAN RONNIE\"]\n",
    "for outlier in outliers :\n",
    "    data_dict.pop(outlier, 0)\n",
    "\n",
    "\n",
    "def update_dict_value(key, items, values, dict_obj):\n",
    "    index = 0\n",
    "    for item in items:     \n",
    "        dict_obj[key][item] = values[index]\n",
    "        index += 1\n",
    "    return dict_obj\n",
    "        \n",
    "\n",
    "    \n",
    "data_dict = update_dict_value(\n",
    "              'BELFER ROBERT',\n",
    "              ['deferred_income','deferral_payments', 'expenses', \n",
    "               'director_fees', 'total_payments', 'exercised_stock_options',\n",
    "               'restricted_stock','restricted_stock_deferred',\n",
    "               'total_stock_value'], \n",
    "              [-102500,'NaN',3285,102500, 3285,'NaN', 44093,-44093,'NaN'],\n",
    "              data_dict)\n",
    "\n",
    "\n",
    "data_dict = update_dict_value(\n",
    "              'BHATNAGAR SANJAY',\n",
    "              ['other', 'expenses', 'director_fees', 'total_payments',\n",
    "               'exercised_stock_options','restricted_stock',\n",
    "               'restricted_stock_deferred','total_stock_value'],\n",
    "              ['NaN',137864, 'NaN', 137864, 15456290, \n",
    "               2604490, -2604490, 15456290],\n",
    "               data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    key_values = data_dict[key]\n",
    "\n",
    "    total_msg = (data_dict[key]['to_messages'] + \n",
    "                 data_dict[key]['from_messages'])\n",
    "    \n",
    "    total_poi_msg = (data_dict[key]['from_poi_to_this_person'] +\n",
    "                     data_dict[key]['from_this_person_to_poi'] + \n",
    "                     data_dict[key]['shared_receipt_with_poi'])     \n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_poi_ratio'] = (float(total_poi_msg) / \n",
    "                                           float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_poi_ratio'] = \"NaN\"\n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_others_ratio'] = ((float(total_msg) - float(total_poi_msg)) / \n",
    "                                          float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_others_ratio'] = \"NaN\"\n",
    "\n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features_list = features_list + email_features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "#features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# transformed version of X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, \n",
    "                                                                                            labels,  \n",
    "                                                                                            test_size=0.3, \n",
    "                                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tje features have quite differente value ranges and some of them are discrete and some of them take continous values, I need to scale them first. Removing mean and dividing the standard deviation o features respectively, this one of the most commonly used preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For now I don't know exactly the number of features I want to use, so I decided to use PCA for identify the most important features and explain the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.861553776249\n",
      "2 : 0.950132015173\n",
      "3 : 0.972382607444\n",
      "4 : 0.984291217765\n",
      "5 : 0.992901638092\n",
      "6 : 0.997163221659\n",
      "7 : 0.998629887029\n",
      "8 : 0.999743067191\n",
      "9 : 0.99987985018\n",
      "10 : 0.999974804369\n",
      "11 : 0.999991386498\n",
      "12 : 1.0\n",
      "13 : 1.0\n",
      "14 : 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    pca = PCA(svd_solver='auto', n_components=i)\n",
    "    x = pca.fit(features_train).explained_variance_ratio_.sum()\n",
    "    print i,\":\", x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this decomposition, the vector array provided by ratio indicates that most of the information is concentrated into the first 2 compontents . You saw this same sort of result after the factor analysis. it's therefore possible to reduce the entire dataset to just two componentes, providing a reduction of noise and redundant information from the original dataset.\n",
    "\n",
    "Let's see which variables are the most important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>variance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>41.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>22.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>12.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>7.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>5.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expenses</td>\n",
       "      <td>4.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>2.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>message_poi_ratio</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>message_others_ratio</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  variance_ratio\n",
       "0                       bonus          41.347\n",
       "1           deferral_payments          22.313\n",
       "2             deferred_income          12.831\n",
       "3               director_fees           7.052\n",
       "4     exercised_stock_options           5.041\n",
       "5                    expenses           4.009\n",
       "6               loan_advances           2.084\n",
       "7         long_term_incentive           1.738\n",
       "8                       other           1.270\n",
       "9            restricted_stock           0.985\n",
       "10  restricted_stock_deferred           0.528\n",
       "11                     salary           0.450\n",
       "12          message_poi_ratio           0.207\n",
       "13       message_others_ratio           0.143"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=14)\n",
    "pca.fit_transform(X_train)\n",
    "pca_df = pd.DataFrame(zip(features_list[1:],\n",
    "                          np.round(pca.explained_variance_ratio_, decimals=5)*100), \n",
    "                      columns=['feature','variance_ratio'])\n",
    "pca_df.sort_values(by='variance_ratio',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't wanna that the features negatively impact my model performance. I'll spend more time on this and try SelecKbest for in this task. \n",
    "\n",
    "First of all, I scaled the data with MinMaxScaler (range 0, 1), then I applyied SelectKBest chi squared statistical test, cause this test need non-negative features. After that I used SelectKBest ANOVA F-value statistical test on the raw data, in a way I could compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest: \n",
      "                      feature      anova      chi2\n",
      "4     exercised_stock_options  22.087532  6.246159\n",
      "0                       bonus  20.524645  5.048256\n",
      "11                     salary  18.003740  2.989183\n",
      "2             deferred_income  11.320185  0.338413\n",
      "12          message_poi_ratio   9.816852  3.371674\n",
      "14          message_poi_ratio   9.816852  3.371674\n",
      "7         long_term_incentive   9.772104  2.497366\n",
      "9            restricted_stock   8.694888  2.463442\n",
      "6               loan_advances   7.125382  6.634816\n",
      "5                    expenses   5.287549  1.293600\n",
      "8                       other   4.143788  1.703679\n",
      "3               director_fees   1.972788  1.508957\n",
      "13       message_others_ratio   1.473822  0.392669\n",
      "15       message_others_ratio   1.473822  0.392669\n",
      "10  restricted_stock_deferred   0.767702  0.008756\n",
      "1           deferral_payments   0.236711  0.094344\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "selector = SelectKBest(chi2, k='all').fit(features_scaled, labels)\n",
    "k_best = SelectKBest(f_classif,k='all').fit(features, labels)    \n",
    "\n",
    "# Format values\n",
    "kbest_pd = pd.DataFrame(zip(features_list[1:],\n",
    "                            k_best.scores_, # K best score                            \n",
    "                            selector.scores_), # chi2\n",
    "                        columns = ['feature','anova', 'chi2'])\n",
    "print \"SelectKBest: \"\n",
    "print kbest_pd.sort_values(by='anova',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing classifier: Naive Bayes\n",
      "Start testing classifier: DecisionTree\n",
      "Start testing classifier: RandomForest\n",
      "Start testing classifier: GradientBoostingClassifier\n",
      "Start testing classifier: AdaBoost\n",
      "Start testing classifier: ExtraTreesClassifier\n",
      "Start testing classifier: LogisticRegression\n",
      "Start testing classifier: KNeighbors\n",
      "Start testing classifier: NearestCentroid\n",
      "                        Model  Precision  Recall  Accuracy\n",
      "7                  KNeighbors   0.705556  0.1270  0.876533\n",
      "2                RandomForest   0.401084  0.1480  0.856933\n",
      "4                    AdaBoost   0.418319  0.3060  0.850733\n",
      "5        ExtraTreesClassifier   0.362011  0.1620  0.850200\n",
      "8             NearestCentroid   0.376596  0.2655  0.843467\n",
      "3  GradientBoostingClassifier   0.263037  0.2295  0.811533\n",
      "1                DecisionTree   0.251683  0.2430  0.802733\n",
      "6          LogisticRegression   0.164218  0.1900  0.763067\n",
      "0                 Naive Bayes   0.219135  0.6230  0.653733\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "# naive_bayes\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "# tree\n",
    "models.append(('DecisionTree', DecisionTreeClassifier(random_state=42)))\n",
    "# ensemble\n",
    "models.append(('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0)))\n",
    "\n",
    "models.append(('AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=100, random_state=42)))\n",
    "# linear_model\n",
    "models.append(('LogisticRegression', LogisticRegression(random_state=42)))\n",
    "# neighbors\n",
    "models.append(('KNeighbors', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('NearestCentroid', NearestCentroid()))\n",
    "\n",
    "accuracy_model = []\n",
    "score = []\n",
    "for name, clf in models:  \n",
    "    tm = time()\n",
    "    #print \"Start testing classifier:\", name\n",
    "\n",
    "    score = test_classifier(clf, my_dataset, features_list)\n",
    "    accuracy_model.append([name,score[2], score[3], score[1]])\n",
    "    \n",
    "scores = pd.DataFrame(accuracy_model,\n",
    "                      columns=('Model', \n",
    "                               'Precision', \n",
    "                               'Recall',\n",
    "                               'Accuracy')).sort_values(by='Accuracy',\n",
    "                                                    ascending = False)\n",
    "\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm interested in the three metrics recall, precision and accuracy in the test with basic parameters, the Models NearestCentroid and GradientBoostingClassifier had a score close to the expected of 0.3. Even without tuning the AdaBoost reached the metric with a precision of 0.41, recall of 0.30 and a good percentage of acuracia of 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the models has reached the goal, but what if we can increase the precision and recall of the other models? Sounds like a good mission, does not it? We, will try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classify', GradientBoostingClassifier(criterion='mae', init=None, learning_rate=1.0,\n",
      "              loss='exponential', max_depth=3, max_features=0.5,\n",
      "              max_leaf_nodes=100, min_impurity_split=1e-07,\n",
      "           ...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.83153\tPrecision: 0.31915\tRecall: 0.23250\tF1: 0.26902\tF2: 0.24585\n",
      "\tTotal predictions: 15000\tTrue positives:  465\tFalse positives:  992\tFalse negatives: 1535\tTrue negatives: 12008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', StandardScaler(with_mean=False)),\n",
    "                     ('classify', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "        #'selection__k': [10],\n",
    "        'classify__criterion': ['mae', 'friedman_mse', 'mse'],\n",
    "        'classify__n_estimators' : [50, 100, 150],\n",
    "        'classify__learning_rate':[0.05, 0.1, 1.0],\n",
    "        #'classify__min_samples_leaf': [3],\n",
    "        #'classify__max_leaf_nodes': [100],\n",
    "        'classify__loss' : ['deviance', 'exponential'],    \n",
    "        'classify__max_leaf_nodes': [100, 150],\n",
    "        'classify__max_features': [0.50] \n",
    "        #'classify__subsample': [0.8, 1.0]        \n",
    "    }\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features,labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\neighbors\\nearest_centroid.py:138: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=0.1))])\n",
      "\tAccuracy: 0.80113\tPrecision: 0.35733\tRecall: 0.61550\tF1: 0.45216\tF2: 0.53779\n",
      "\tTotal predictions: 15000\tTrue positives: 1231\tFalse positives: 2214\tFalse negatives:  769\tTrue negatives: 10786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                           ('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('classifier', NearestCentroid())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'scale' : [StandardScaler(with_mean=False)],\n",
    "              'reduce_dim__n_components': [6],\n",
    "              'classifier__metric': ['cityblock', 'euclidean', 'l1', 'l2', 'manhattan'],\n",
    "              'classifier__shrink_threshold'  : [0.001, 0.01, 0.1, 1.0]\n",
    "         }\n",
    "\n",
    "scv = StratifiedShuffleSplit(labels_train, 1000, random_state = 42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=scv, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Last but not least...  AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll tuning the classify that I'll use after as base estimator on Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=0.5, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "\tAccuracy: 0.69293\tPrecision: 0.26790\tRecall: 0.75200\tF1: 0.39506\tF2: 0.55237\n",
      "\tTotal predictions: 15000\tTrue positives: 1504\tFalse positives: 4110\tFalse negatives:  496\tTrue negatives: 8890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "                          ])\n",
    "params = {\n",
    "          'classifier__max_features': [2, 6, 10],\n",
    "          'classifier__min_samples_split' : [0.1, .50, .90],\n",
    "          'classifier__splitter' : ['best' , 'random'],\n",
    "          'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "          'classifier__class_weight' : ['balanced', None],\n",
    "          'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf_bs = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf_bs, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurit...ndom_state=42, splitter='random'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.80280\tPrecision: 0.36377\tRecall: 0.63950\tF1: 0.46374\tF2: 0.55531\n",
      "\tTotal predictions: 15000\tTrue positives: 1279\tFalse positives: 2237\tFalse negatives:  721\tTrue negatives: 10763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           #('reduce',PCA(n_components=6, random_state=42)),\n",
    "                           ('classifier', AdaBoostClassifier(random_state=42))\n",
    "                               ])\n",
    "params = {\n",
    "          #'selection__k': [14],\n",
    "          'classifier__base_estimator' : [DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
    "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=0.5, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=42, splitter='random')], \n",
    "          'classifier__n_estimators': [150,200],          \n",
    "          'classifier__learning_rate' :[0.01, 0.05, 0.1, 1.0],\n",
    "          'classifier__algorithm' : ['SAMME.R', 'SAMME']\n",
    "               }\n",
    "\n",
    "#scv = StratifiedShuffleSplit(features_train, 1000, random_state = 42)\n",
    "\n",
    "# set up gridsearch\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now I'm a bit curious...\n",
    "about how much other models can be improved. Let's continue the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.86080\tPrecision: 0.43103\tRecall: 0.13750\tF1: 0.20849\tF2: 0.15918\n",
      "\tTotal predictions: 15000\tTrue positives:  275\tFalse positives:  363\tFalse negatives: 1725\tTrue negatives: 12637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline =   Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                       #('select', SelectKBest()),   \n",
    "                       ('classifier', RandomForestClassifier())\n",
    "                      ])\n",
    "\n",
    "param_grid = {'scale': [None, MaxAbsScaler(), MinMaxScaler(feature_range=(0, 1))],\n",
    "              #'select__k': [6, 10],\n",
    "              'classifier__max_depth': [5, 3, 1],\n",
    "              'classifier__max_features': [2],\n",
    "              'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "              'classifier__bootstrap': [True, False],\n",
    "              'classifier__criterion': ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf =  grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='distance'))])\n",
      "\tAccuracy: 0.83980\tPrecision: 0.39456\tRecall: 0.37700\tF1: 0.38558\tF2: 0.38039\n",
      "\tTotal predictions: 15000\tTrue positives:  754\tFalse positives: 1157\tFalse negatives: 1246\tTrue negatives: 11843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('scale', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', KNeighborsClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'scale' : [None, StandardScaler(with_mean=False)],\n",
    "              #'selection__k': [6, 10, 14],\n",
    "              #'reduce_dim__n_components': [10],\n",
    "              'classifier__metric': ['minkowski','euclidean'],\n",
    "              'classifier__n_neighbors' : range(2,14,2),\n",
    "              'classifier__weights' : ['uniform','distance']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C695668>)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.80133\tPrecision: 0.35250\tRecall: 0.58550\tF1: 0.44006\tF2: 0.51713\n",
      "\tTotal predictions: 15000\tTrue positives: 1171\tFalse positives: 2151\tFalse negatives:  829\tTrue negatives: 10849\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=2, max_leaf_nodes=100, min_impurity_spl...one,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.82233\tPrecision: 0.32509\tRecall: 0.30900\tF1: 0.31684\tF2: 0.31209\n",
      "\tTotal predictions: 15000\tTrue positives:  618\tFalse positives: 1283\tFalse negatives: 1382\tTrue negatives: 11717\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b0c50803f744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\lieby\\Documents\\tester.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(clf, dataset, feature_list, folds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;31m### fit the classifier using training set, and test on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[0;32m--> 314\u001b[0;31m                                             random_state=random_state)\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\base.pyc\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[1;32m    121\u001b[0m                                     for p in self.estimator_params))\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m                             % (repr(estimator), type(estimator)))\n\u001b[1;32m     66\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[1;31m# This is set in utils/__init__.py but it gets overwritten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[1;31m# when running under python3 somehow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"always\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\warnings.pyc\u001b[0m in \u001b[0;36msimplefilter\u001b[0;34m(action, category, lineno, append)\u001b[0m\n\u001b[1;32m    106\u001b[0m     assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n\u001b[1;32m    107\u001b[0m                       \"once\"), \"invalid action: %r\" % (action,)\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlineno\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m            \u001b[1;34m\"lineno must be an int >= 0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('selection', SelectKBest()),\n",
    "                           ('classifier', RandomForestClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, MinMaxScaler(feature_range=(0, 1))],\n",
    "              'selection__k': [6, 10, 'all'],\n",
    "              #'reduce_dim__n_components': [6],\n",
    "              'classifier__n_estimators': [100, 150, 200],\n",
    "              'classifier__criterion': ['gini', 'entropy'],\n",
    "              'classifier__max_depth': [5,10],\n",
    "              'classifier__min_samples_split': [3,2] ,\n",
    "              'classifier__bootstrap': [True, False]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=500, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.76647\tPrecision: 0.30177\tRecall: 0.57200\tF1: 0.39510\tF2: 0.48512\n",
      "\tTotal predictions: 15000\tTrue positives: 1144\tFalse positives: 2647\tFalse negatives:  856\tTrue negatives: 10353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', LogisticRegression(random_state=42))\n",
    "                          ])\n",
    "\n",
    "param_grid = {#'minmaxer' : [StandardScaler(with_mean=False), None],\n",
    "          #'selection__k': [10, 'all'],\n",
    "          'classifier__C': [0.05, 0.5, 1, 10, 100, 500, 1000],\n",
    "          'classifier__solver': ['liblinear'],\n",
    "          'classifier__penalty': ['l2'], \n",
    "          'classifier__class_weight': ['balanced']\n",
    "         }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='gini', max_depth=None, max_features=2,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=10, min_samples_split=0.1,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.63640\tPrecision: 0.22082\tRecall: 0.68300\tF1: 0.33374\tF2: 0.48146\n",
      "\tTotal predictions: 15000\tTrue positives: 1366\tFalse positives: 4820\tFalse negatives:  634\tTrue negatives: 8180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "                          ])\n",
    "params = {\n",
    "          'classifier__max_features': [2, 6, 10],\n",
    "          'classifier__min_samples_split' : [0.1, .50, .90],\n",
    "          'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "          'classifier__class_weight' : ['balanced', None],\n",
    "          'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf_bs = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf_bs, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85053\tPrecision: 0.42234\tRecall: 0.32900\tF1: 0.36987\tF2: 0.34421\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  900\tFalse negatives: 1342\tTrue negatives: 12100\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85060\tPrecision: 0.42261\tRecall: 0.32900\tF1: 0.36997\tF2: 0.34425\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  899\tFalse negatives: 1342\tTrue negatives: 12101\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tunning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "http://www.ritchieng.com/machine-learning-project-student-intervention/\n",
    "https://github.com/baumanab/udacity_intro_machinelearning_project/blob/master/final_project/my_poi_id.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
