{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron - How many POIs we can predict?\n",
    "\n",
    "### Index\n",
    "\n",
    "1. Set variables\n",
    "2. Data cleansing\n",
    "3. New features\n",
    "4. Preparing the data for the split\n",
    "5. Spliting data into train and test\n",
    "6. Selecting features\n",
    "7. Testing models\n",
    "8. Tuning Algorithm Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "    \n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import  MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "#Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest,chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Feature selection\n",
    "from sklearn.decomposition import PCA,RandomizedPCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest,chi2, f_regression\n",
    "\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the next steps the data will be cleaned, if you want to understand the process, check: \n",
    "https://github.com/liebycardoso/ML_Enron/blob/master/Enron_Data_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_label = 'poi'                \n",
    "\n",
    "financial_features_list = [\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary'\n",
    "]\n",
    "\n",
    "features_list = [target_label] + financial_features_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"P:/Nanodegree/ML/ud120-projects/tools/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "outliers = [\"TOTAL\", \"THE TRAVEL AGENCY IN THE PARK\", \"LOCKHART EUGENE E\", \"CHAN RONNIE\"]\n",
    "for outlier in outliers :\n",
    "    data_dict.pop(outlier, 0)\n",
    "\n",
    "\n",
    "def update_dict_value(key, items, values, dict_obj):\n",
    "    index = 0\n",
    "    for item in items:     \n",
    "        dict_obj[key][item] = values[index]\n",
    "        index += 1\n",
    "    return dict_obj       \n",
    "\n",
    "    \n",
    "data_dict = update_dict_value(\n",
    "              'BELFER ROBERT',\n",
    "              ['deferred_income','deferral_payments', 'expenses', \n",
    "               'director_fees', 'total_payments', 'exercised_stock_options',\n",
    "               'restricted_stock','restricted_stock_deferred',\n",
    "               'total_stock_value'], \n",
    "              [-102500,'NaN',3285,102500, 3285,'NaN', 44093,-44093,'NaN'],\n",
    "              data_dict)\n",
    "\n",
    "\n",
    "data_dict = update_dict_value(\n",
    "              'BHATNAGAR SANJAY',\n",
    "              ['other', 'expenses', 'director_fees', 'total_payments',\n",
    "               'exercised_stock_options','restricted_stock',\n",
    "               'restricted_stock_deferred','total_stock_value'],\n",
    "              ['NaN',137864, 'NaN', 137864, 15456290, \n",
    "               2604490, -2604490, 15456290],\n",
    "               data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    key_values = data_dict[key]\n",
    "\n",
    "    total_msg = (data_dict[key]['to_messages'] + \n",
    "                 data_dict[key]['from_messages'])\n",
    "    \n",
    "    total_poi_msg = (data_dict[key]['from_poi_to_this_person'] +\n",
    "                     data_dict[key]['from_this_person_to_poi'] + \n",
    "                     data_dict[key]['shared_receipt_with_poi'])     \n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_poi_ratio'] = (float(total_poi_msg) / \n",
    "                                           float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_poi_ratio'] = \"NaN\"\n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_others_ratio'] = ((float(total_msg) - float(total_poi_msg)) / \n",
    "                                          float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_others_ratio'] = \"NaN\"\n",
    "\n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing the data for the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Spliting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# transformed version of X\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, \n",
    "                                                                                            labels,  \n",
    "                                                                                            test_size=0.3, \n",
    "                                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tje features have quite differente value ranges and some of them are discrete and some of them take continous values, I need to scale them first. Removing mean and dividing the standard deviation o features respectively, this one of the most commonly used preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For now I don't know exactly the number of features I want to use, so I decided to use PCA for identify the most important features and explain the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.861553776249\n",
      "2 : 0.950132015173\n",
      "3 : 0.972382607444\n",
      "4 : 0.984291217765\n",
      "5 : 0.992901638092\n",
      "6 : 0.997163221659\n",
      "7 : 0.998629887029\n",
      "8 : 0.999743067191\n",
      "9 : 0.99987985018\n",
      "10 : 0.999974804369\n",
      "11 : 0.999991386498\n",
      "12 : 1.0\n",
      "13 : 1.0\n",
      "14 : 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    pca = PCA(svd_solver='auto', n_components=i)\n",
    "    x = pca.fit(features_train).explained_variance_ratio_.sum()\n",
    "    print i,\":\", x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this decomposition, the vector array provided by ratio indicates that most of the information is concentrated into the first 2 compontents . You saw this same sort of result after the factor analysis. it's therefore possible to reduce the entire dataset to just two componentes, providing a reduction of noise and redundant information from the original dataset.\n",
    "\n",
    "Let's see which variables are the most important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>variance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>86.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>8.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>2.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>1.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expenses</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>message_poi_ratio</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>message_others_ratio</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  variance_ratio\n",
       "0                       bonus          86.155\n",
       "1           deferral_payments           8.858\n",
       "2             deferred_income           2.225\n",
       "3               director_fees           1.191\n",
       "4     exercised_stock_options           0.861\n",
       "5                    expenses           0.426\n",
       "6               loan_advances           0.147\n",
       "7         long_term_incentive           0.111\n",
       "8                       other           0.014\n",
       "9            restricted_stock           0.009\n",
       "10  restricted_stock_deferred           0.002\n",
       "11                     salary           0.001\n",
       "12          message_poi_ratio           0.000\n",
       "13       message_others_ratio           0.000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=14)\n",
    "pca.fit_transform(features_train)\n",
    "pca_df = pd.DataFrame(zip(features_list[1:],\n",
    "                          np.round(pca.explained_variance_ratio_, decimals=5)*100), \n",
    "                      columns=['feature','variance_ratio'])\n",
    "pca_df.sort_values(by='variance_ratio',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't wanna that the features negatively impact my model performance. I'll spend more time on this and try SelecKbest for in this task. \n",
    "\n",
    "First of all, I scaled the data with MinMaxScaler (range 0, 1), then I applyied SelectKBest chi squared statistical test, cause this test need non-negative features. After that I used SelectKBest ANOVA F-value statistical test on the raw data, in a way I could compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest: \n",
      "                      feature      anova      chi2\n",
      "4     exercised_stock_options  22.087532  6.246159\n",
      "0                       bonus  20.524645  5.048256\n",
      "11                     salary  18.003740  2.989183\n",
      "2             deferred_income  11.320185  0.338413\n",
      "12          message_poi_ratio   9.816852  3.371674\n",
      "7         long_term_incentive   9.772104  2.497366\n",
      "9            restricted_stock   8.694888  2.463442\n",
      "6               loan_advances   7.125382  6.634816\n",
      "5                    expenses   5.287549  1.293600\n",
      "8                       other   4.143788  1.703679\n",
      "3               director_fees   1.972788  1.508957\n",
      "13       message_others_ratio   1.473822  0.392669\n",
      "10  restricted_stock_deferred   0.767702  0.008756\n",
      "1           deferral_payments   0.236711  0.094344\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "selector = SelectKBest(chi2, k='all').fit(features_scaled, labels)\n",
    "k_best = SelectKBest(f_classif,k='all').fit(features, labels)    \n",
    "\n",
    "# Format values\n",
    "kbest_pd = pd.DataFrame(zip(features_list[1:],\n",
    "                            k_best.scores_, # K best score                            \n",
    "                            selector.scores_), # chi2\n",
    "                        columns = ['feature','anova', 'chi2'])\n",
    "print \"SelectKBest: \"\n",
    "print kbest_pd.sort_values(by='anova',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The two metrics returned similar values of importance for the features. They differed only in the loan_advances and salary records.\n",
    "I'm happy to know that the feature I created message_poi_ratio got a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Model  Precision  Recall  Accuracy\n",
      "7                  KNeighbors   0.705556  0.1270  0.876533\n",
      "2                RandomForest   0.441931  0.1465  0.861533\n",
      "5        ExtraTreesClassifier   0.411685  0.1515  0.858000\n",
      "4                    AdaBoost   0.418319  0.3060  0.850733\n",
      "8             NearestCentroid   0.376596  0.2655  0.843467\n",
      "3  GradientBoostingClassifier   0.260177  0.2205  0.812467\n",
      "1                DecisionTree   0.252969  0.2450  0.802867\n",
      "6          LogisticRegression   0.164218  0.1900  0.763067\n",
      "0                 Naive Bayes   0.219135  0.6230  0.653733\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "# naive_bayes\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "# tree\n",
    "models.append(('DecisionTree', DecisionTreeClassifier(random_state=42)))\n",
    "# ensemble\n",
    "models.append(('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0)))\n",
    "\n",
    "models.append(('AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=100, random_state=42)))\n",
    "# linear_model\n",
    "models.append(('LogisticRegression', LogisticRegression(random_state=42)))\n",
    "# neighbors\n",
    "models.append(('KNeighbors', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('NearestCentroid', NearestCentroid()))\n",
    "\n",
    "accuracy_model = []\n",
    "score = []\n",
    "for name, clf in models:  \n",
    "    tm = time()\n",
    "    #print \"Start testing classifier:\", name\n",
    "\n",
    "    score = test_classifier(clf, my_dataset, features_list)\n",
    "    accuracy_model.append([name,score[2], score[3], score[1]])\n",
    "    \n",
    "scores = pd.DataFrame(accuracy_model,\n",
    "                      columns=('Model', \n",
    "                               'Precision', \n",
    "                               'Recall',\n",
    "                               'Accuracy')).sort_values(by='Accuracy',\n",
    "                                                    ascending = False)\n",
    "\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm interested in the three metrics recall, precision and accuracy in the test with basic parameters, the Models NearestCentroid and GradientBoostingClassifier had a score close to the expected of 0.3. Even without tuning the AdaBoost reached the metric with a precision of 0.41, recall of 0.30 and a good percentage of acuracia of 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the models has reached the goal, but what if we can increase the precision and recall of the other models? Sounds like a good mission, does not it? We, will try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tuning Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classify', GradientBoostingClassifier(criterion='mae', init=None, learning_rate=1.0,\n",
      "              loss='exponential', max_depth=3, max_features=0.5,\n",
      "              max_leaf_nodes=100, min_impurity_split=1e-07,\n",
      "           ...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.83473\tPrecision: 0.31391\tRecall: 0.20200\tF1: 0.24582\tF2: 0.21751\n",
      "\tTotal predictions: 15000\tTrue positives:  404\tFalse positives:  883\tFalse negatives: 1596\tTrue negatives: 12117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', StandardScaler(with_mean=False)),\n",
    "                     ('classify', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "        'classify__criterion': ['mae', 'friedman_mse', 'mse'],\n",
    "        'classify__n_estimators' : [50, 100, 150],\n",
    "        'classify__learning_rate':[0.05, 0.1, 1.0],\n",
    "        'classify__loss' : ['deviance', 'exponential'],    \n",
    "        'classify__max_leaf_nodes': [100, 150],\n",
    "        'classify__max_features': [0.50]       \n",
    "    }\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features,labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After a scale and a search for best params with gridsearchCV we improve all of three scores, but I can't  able to achieve a recall greater than 0.30.\n",
    "\n",
    "| Model                        | Precision |    Recall | Accuracy  |\n",
    "|------------------------------|-----------|-----------|-----------|\n",
    "|GradientBoosting before       |   0.26017 |     0.2205|    0.81246|\n",
    "|GradientBoosting after tuning |   0.31391 |     0.2020|    0.83473|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\neighbors\\nearest_centroid.py:138: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', NearestCentroid(metric='cityblock', shrink_threshold=0.001))])\n",
      "\tAccuracy: 0.80793\tPrecision: 0.34899\tRecall: 0.50900\tF1: 0.41407\tF2: 0.46625\n",
      "\tTotal predictions: 15000\tTrue positives: 1018\tFalse positives: 1899\tFalse negatives:  982\tTrue negatives: 11101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                           ('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('classifier', NearestCentroid())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'scale' : [StandardScaler(with_mean=False)],\n",
    "              'reduce_dim__n_components': [6],\n",
    "              'classifier__metric': ['cityblock', 'euclidean', 'l1', 'l2', 'manhattan'],\n",
    "              'classifier__shrink_threshold'  : [0.001, 0.01, 0.1, 1.0]\n",
    "         }\n",
    "\n",
    "#scv = StratifiedShuffleSplit(labels_train, 1000, random_state = 42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now I'm happier with the result, although it has reduced Precision and accuracy a bit, there was a significant increase in recall.\n",
    "\n",
    "| Model                        | Precision |    Recall | Accuracy  |\n",
    "|------------------------------|-----------|-----------|-----------|\n",
    "|NearestCentroid before        |    0.37659|     0.2655|    0.84346|\n",
    "|NearestCentroid after tuning  |    0.34899|     0.5090|    0.80793|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Last but not least...  AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll tuning the classify that I'll use after as base estimator on Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=0.5, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "\tAccuracy: 0.69293\tPrecision: 0.26790\tRecall: 0.75200\tF1: 0.39506\tF2: 0.55237\n",
      "\tTotal predictions: 15000\tTrue positives: 1504\tFalse positives: 4110\tFalse negatives:  496\tTrue negatives: 8890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "                          ])\n",
    "params = {\n",
    "          'classifier__max_features': [2, 6, 10],\n",
    "          'classifier__min_samples_split' : [0.1, .50, .90],\n",
    "          'classifier__splitter' : ['best' , 'random'],\n",
    "          'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "          'classifier__class_weight' : ['balanced', None],\n",
    "          'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf_bs = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf_bs, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurit...ndom_state=42, splitter='random'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.80280\tPrecision: 0.36377\tRecall: 0.63950\tF1: 0.46374\tF2: 0.55531\n",
      "\tTotal predictions: 15000\tTrue positives: 1279\tFalse positives: 2237\tFalse negatives:  721\tTrue negatives: 10763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           #('reduce',PCA(n_components=6, random_state=42)),\n",
    "                           ('classifier', AdaBoostClassifier(random_state=42))\n",
    "                               ])\n",
    "params = {\n",
    "          #'selection__k': [14],\n",
    "          'classifier__base_estimator': [DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
    "                                        max_depth=None, max_features=2, max_leaf_nodes=None,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        min_samples_split=0.5, min_weight_fraction_leaf=0.0,\n",
    "                                        presort=False, random_state=42, splitter='random')], \n",
    "          'classifier__n_estimators': [150,200],          \n",
    "          'classifier__learning_rate' :[0.01, 0.05, 0.1, 1.0],\n",
    "          'classifier__algorithm' : ['SAMME.R', 'SAMME']\n",
    "               }\n",
    "\n",
    "# You can use StratifiedShuffleSplit\n",
    "#scv = StratifiedShuffleSplit(features_train, 1000, random_state = 42)\n",
    "\n",
    "# set up gridsearch\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got it! I think I found the one ;)\n",
    "\n",
    "I tried to reduce dimensionality but did not succeed. So I chose not to use PCA or Kbest.\n",
    "This model achieves the goal of a score greater than 0.30 for recall and precision.\n",
    "\n",
    "\n",
    "| Model                           | Precision |    Recall | Accuracy  |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "|AdaBoostClassifier before        |    0.41831|     0.3060|    0.85073|\n",
    "|AdaBoostClassifier after tuning  |    0.36377|     0.6395|    0.80280|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now I'm a bit curious...\n",
    "about how much other models can be improved. Let's continue the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.86080\tPrecision: 0.43103\tRecall: 0.13750\tF1: 0.20849\tF2: 0.15918\n",
      "\tTotal predictions: 15000\tTrue positives:  275\tFalse positives:  363\tFalse negatives: 1725\tTrue negatives: 12637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline =   Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                       ('classifier', RandomForestClassifier())\n",
    "                      ])\n",
    "\n",
    "param_grid = {'scale': [None, MaxAbsScaler(), MinMaxScaler(feature_range=(0, 1))],\n",
    "              'classifier__max_depth': [5, 3, 1],\n",
    "              'classifier__max_features': [2],\n",
    "              'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "              'classifier__bootstrap': [True, False],\n",
    "              'classifier__criterion': ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf =  grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement:\n",
    "\n",
    "| Model                           | Precision |    Recall | Accuracy  |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "|RandomForest before              |    0.44193|     0.1465|    0.86153|\n",
    "|RandomForest after tuning        |    0.43103|     0.1375|    0.86080|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='distance'))])\n",
      "\tAccuracy: 0.83980\tPrecision: 0.39456\tRecall: 0.37700\tF1: 0.38558\tF2: 0.38039\n",
      "\tTotal predictions: 15000\tTrue positives:  754\tFalse positives: 1157\tFalse negatives: 1246\tTrue negatives: 11843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('scale', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', KNeighborsClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'scale' : [None, StandardScaler(with_mean=False)],\n",
    "              #'selection__k': [6, 10, 14],\n",
    "              #'reduce_dim__n_components': [10],\n",
    "              'classifier__metric': ['minkowski','euclidean'],\n",
    "              'classifier__n_neighbors' : range(2,14,2),\n",
    "              'classifier__weights' : ['uniform','distance']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier had a decrease in the recall score. Now it has a more balanced values for precision and recall.\n",
    "\n",
    "| Model                           | Precision |    Recall | Accuracy  |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "|KNeighbors before                |    0.70555|     0.1270|    0.87653|\n",
    "|KNeighbors after tuning          |    0.39456|     0.3770|    0.83980|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=500, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.76647\tPrecision: 0.30177\tRecall: 0.57200\tF1: 0.39510\tF2: 0.48512\n",
      "\tTotal predictions: 15000\tTrue positives: 1144\tFalse positives: 2647\tFalse negatives:  856\tTrue negatives: 10353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', LogisticRegression(random_state=42))\n",
    "                          ])\n",
    "\n",
    "param_grid = {#'minmaxer' : [StandardScaler(with_mean=False), None],\n",
    "              #'selection__k': [10, 'all'],\n",
    "              'classifier__C': [0.05, 0.5, 1, 10, 100, 500, 1000],\n",
    "              'classifier__solver': ['liblinear'],\n",
    "              'classifier__penalty': ['l2'], \n",
    "              'classifier__class_weight': ['balanced']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I really liked the improvement achieved in this classifier after tuning.\n",
    "\n",
    "| Model                           | Precision |    Recall | Accuracy  |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "|LogisticRegression  before       |    0.16421|     0.1900|    0.76306|\n",
    "|LogisticRegression  after tuning |    0.30177|     0.5720|    0.76647|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='gini', max_depth=None, max_features=2,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=10, min_samples_split=0.1,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.63640\tPrecision: 0.22082\tRecall: 0.68300\tF1: 0.33374\tF2: 0.48146\n",
      "\tTotal predictions: 15000\tTrue positives: 1366\tFalse positives: 4820\tFalse negatives:  634\tTrue negatives: 8180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "                          ])\n",
    "params = {\n",
    "          'classifier__max_features': [2, 6, 10],\n",
    "          'classifier__min_samples_split' : [0.1, .50, .90],\n",
    "          'classifier__min_samples_leaf': [1, 3, 10,15],\n",
    "          'classifier__class_weight' : ['balanced', None],\n",
    "          'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'recall', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf_bs = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf_bs, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
