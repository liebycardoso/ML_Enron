{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit\n",
    "    \n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import  MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Feature selection\n",
    "from sklearn.decomposition import PCA,RandomizedPCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest,SelectPercentile\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,classification_report\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_label = 'poi'                \n",
    "email_features_list = [\n",
    "    # 'email_address', # remit email address; informational label\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    ]\n",
    "\n",
    "financial_features_list = [\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary'\n",
    "    #,\n",
    "    #'total_payments',\n",
    "    #'total_stock_value',\n",
    "]\n",
    "features_list = [target_label] + financial_features_list  \n",
    "#features_list = [target_label] + financial_features_list  + email_features_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"P:/Nanodegree/ML/ud120-projects/tools/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = [\"TOTAL\", \"THE TRAVEL AGENCY IN THE PARK\", \"LOCKHART EUGENE E\", \"CHAN RONNIE\"]\n",
    "for outlier in outliers :\n",
    "    data_dict.pop(outlier, 0)\n",
    "\n",
    "\n",
    "def update_dict_value(key, items, values, dict_obj):\n",
    "    index = 0\n",
    "    for item in items:     \n",
    "        dict_obj[key][item] = values[index]\n",
    "        index += 1\n",
    "    return dict_obj\n",
    "        \n",
    "\n",
    "    \n",
    "data_dict = update_dict_value(\n",
    "              'BELFER ROBERT',\n",
    "              ['deferred_income','deferral_payments', 'expenses', \n",
    "               'director_fees', 'total_payments', 'exercised_stock_options',\n",
    "               'restricted_stock','restricted_stock_deferred',\n",
    "               'total_stock_value'], \n",
    "              [-102500,'NaN',3285,102500, 3285,'NaN', 44093,-44093,'NaN'],\n",
    "              data_dict)\n",
    "\n",
    "\n",
    "data_dict = update_dict_value(\n",
    "              'BHATNAGAR SANJAY',\n",
    "              ['other', 'expenses', 'director_fees', 'total_payments',\n",
    "               'exercised_stock_options','restricted_stock',\n",
    "               'restricted_stock_deferred','total_stock_value'],\n",
    "              ['NaN',137864, 'NaN', 137864, 15456290, \n",
    "               2604490, -2604490, 15456290],\n",
    "               data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    key_values = data_dict[key]\n",
    "\n",
    "    total_msg = (data_dict[key]['to_messages'] + \n",
    "                 data_dict[key]['from_messages'])\n",
    "    \n",
    "    total_poi_msg = (data_dict[key]['from_poi_to_this_person'] +\n",
    "                     data_dict[key]['from_this_person_to_poi'] + \n",
    "                     data_dict[key]['shared_receipt_with_poi'])     \n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_poi_ratio'] = (float(total_poi_msg) / \n",
    "                                           float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_poi_ratio'] = \"NaN\"\n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_others_ratio'] = ((float(total_msg) - float(total_poi_msg)) / \n",
    "                                          float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_others_ratio'] = \"NaN\"\n",
    "\n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features_list = features_list + email_features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# transformed version of X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, \n",
    "                                                                                            labels,  \n",
    "                                                                                            test_size=0.3, \n",
    "                                                                                            random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scaled, \n",
    "                                                                    labels,  \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tje features have quite differente value ranges and some of them are discrete and some of them take continous values, I need to scale them first. Removing mean and dividing the standard deviation o features respectively, this one of the most commonly used preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99997223540161428"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA,RandomizedPCA, TruncatedSVD\n",
    "TruncatedSVD(n_components=10).fit(features_train).explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import classification_report\\n#target_names = [\"campo1\"]\\nprint(classification_report(y_test, preds))\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.metrics import classification_report\n",
    "#target_names = [\"campo1\"]\n",
    "print(classification_report(y_test, preds))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>variance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>39.25279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>21.03506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>11.36320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>6.63091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>5.10120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expenses</td>\n",
       "      <td>4.43445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>3.20898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>1.80918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>1.63966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>1.23159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>1.02020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.82485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>message_poi_ratio</td>\n",
       "      <td>0.73930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>message_others_ratio</td>\n",
       "      <td>0.62078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  variance_ratio\n",
       "0                       bonus        39.25279\n",
       "1           deferral_payments        21.03506\n",
       "2             deferred_income        11.36320\n",
       "3               director_fees         6.63091\n",
       "4     exercised_stock_options         5.10120\n",
       "5                    expenses         4.43445\n",
       "6               loan_advances         3.20898\n",
       "7         long_term_incentive         1.80918\n",
       "8                       other         1.63966\n",
       "9            restricted_stock         1.23159\n",
       "10  restricted_stock_deferred         1.02020\n",
       "11                     salary         0.82485\n",
       "12          message_poi_ratio         0.73930\n",
       "13       message_others_ratio         0.62078"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=14)\n",
    "pca.fit_transform(X_train)\n",
    "pca_df = pd.DataFrame(zip(features_list[1:],\n",
    "                          np.round(pca.explained_variance_ratio_, decimals=7)*100), \n",
    "                      columns=['feature','variance_ratio'])\n",
    "pca_df.sort_values(by='variance_ratio',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.861553701723\n",
      "2 : 0.950131933471\n",
      "3 : 0.972382524142\n",
      "4 : 0.984291137831\n",
      "5 : 0.992901577692\n",
      "6 : 0.997163161022\n",
      "7 : 0.998629826373\n",
      "8 : 0.999743007011\n",
      "9 : 0.999879791085\n",
      "10 : 0.99997474527\n",
      "11 : 0.999991327915\n",
      "12 : 0.999999941617\n",
      "13 : 0.999999986864\n",
      "14 : 0.999999999037\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    pca = PCA(svd_solver='auto', n_components=i)\n",
    "    x = pca.fit(features_train).explained_variance_ratio_.sum()\n",
    "    print i,\":\", x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this decomposition, the vector array provided by ratio indicates that most of the information is concentrated into the first compontent(x%). You saw this same sort of result after the factor analysis. it's therefore possible to reduce the entire dataset to just two componentes, providing a reduction of noise and redundant information from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>anova_scaled</th>\n",
       "      <th>anova</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>36.778900</td>\n",
       "      <td>36.778900</td>\n",
       "      <td>6.683411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>message_poi_ratio</td>\n",
       "      <td>16.448332</td>\n",
       "      <td>16.448332</td>\n",
       "      <td>5.439881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salary</td>\n",
       "      <td>16.279541</td>\n",
       "      <td>16.279541</td>\n",
       "      <td>2.923780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>7.836464</td>\n",
       "      <td>7.836464</td>\n",
       "      <td>2.236481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>6.954889</td>\n",
       "      <td>6.954889</td>\n",
       "      <td>6.369138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>6.468488</td>\n",
       "      <td>6.468488</td>\n",
       "      <td>1.797035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>6.298692</td>\n",
       "      <td>6.298692</td>\n",
       "      <td>0.149035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expenses</td>\n",
       "      <td>5.562326</td>\n",
       "      <td>5.562326</td>\n",
       "      <td>1.251584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>4.920009</td>\n",
       "      <td>4.920009</td>\n",
       "      <td>1.721534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>2.716646</td>\n",
       "      <td>2.716646</td>\n",
       "      <td>1.243399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>1.767123</td>\n",
       "      <td>1.767123</td>\n",
       "      <td>1.275959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>1.138212</td>\n",
       "      <td>1.138212</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>message_others_ratio</td>\n",
       "      <td>0.198870</td>\n",
       "      <td>0.198870</td>\n",
       "      <td>0.052897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  anova_scaled      anova      chi2\n",
       "0                       bonus     36.778900  36.778900  6.683411\n",
       "12          message_poi_ratio     16.448332  16.448332  5.439881\n",
       "11                     salary     16.279541  16.279541  2.923780\n",
       "4     exercised_stock_options      7.836464   7.836464  2.236481\n",
       "6               loan_advances      6.954889   6.954889  6.369138\n",
       "7         long_term_incentive      6.468488   6.468488  1.797035\n",
       "2             deferred_income      6.298692   6.298692  0.149035\n",
       "5                    expenses      5.562326   5.562326  1.251584\n",
       "9            restricted_stock      4.920009   4.920009  1.721534\n",
       "8                       other      2.716646   2.716646  1.243399\n",
       "3               director_fees      1.767123   1.767123  1.275959\n",
       "10  restricted_stock_deferred      1.138212   1.138212  0.001669\n",
       "13       message_others_ratio      0.198870   0.198870  0.052897\n",
       "1           deferral_payments      0.001801   0.001801  0.000841"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2, f_regression\n",
    "selector = SelectKBest(chi2, k='all').fit(X_train, y_train)\n",
    "\n",
    "# ANOVA F-value between label/feature for classification tasks\n",
    "k_best = SelectKBest(f_regression,k='all').fit(features_train, labels_train)\n",
    "k_best_scaled = SelectKBest(k='all').fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Format values\n",
    "kbest_pd = pd.DataFrame(zip(features_list[1:], \n",
    "                            k_best_scaled.scores_, # scaled\n",
    "                            k_best.scores_, # K best score                            \n",
    "                            selector.scores_), # chi2\n",
    "                        columns = ['feature','anova_scaled','anova', 'chi2'])\n",
    "\n",
    "kbest_pd.sort_values(by='anova_scaled',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.95      0.92        38\n",
      "        1.0       0.33      0.20      0.25         5\n",
      "\n",
      "avg / total       0.83      0.86      0.84        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select = SelectKBest(k=10)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "steps = [('feature_selection', select),\n",
    "        ('random_forest', clf)]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(features, \n",
    "#                                                                             labels, test_size=0.33, random_state=42)\n",
    "\n",
    "### fit your pipeline on X_train and y_train\n",
    "pipeline.fit( X_train, y_train )\n",
    "### call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "y_prediction = pipeline.predict( X_test )\n",
    "### test your predictions using sklearn.classification_report()\n",
    "report = classification_report( y_test, y_prediction )\n",
    "### and print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>Score3</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.873227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.866874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.852394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.781767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.774823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.674645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model    Score1    Score2    Score3      Mean\n",
       "3           KNeighbors  0.875000  0.872340  0.872340  0.873227\n",
       "4             AdaBoost  0.770833  0.914894  0.914894  0.866874\n",
       "2        Random Forest  0.812500  0.851064  0.893617  0.852394\n",
       "5  Logistic Regression  0.770833  0.765957  0.808511  0.781767\n",
       "1        Decision Tree  0.750000  0.744681  0.829787  0.774823\n",
       "0          Naive Bayes  0.875000  0.765957  0.382979  0.674645"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = [GaussianNB(), \n",
    "              DecisionTreeClassifier(), \n",
    "              RandomForestClassifier(), \n",
    "              KNeighborsClassifier(n_neighbors=4),\n",
    "              AdaBoostClassifier(),\n",
    "              LogisticRegression()]\n",
    "classifier_name = ['Naive Bayes', \n",
    "                   'Decision Tree', \n",
    "                   'Random Forest',\n",
    "                   'KNeighbors',\n",
    "                   'AdaBoost',\n",
    "                   'Logistic Regression']\n",
    "\n",
    "accuracy_model = []\n",
    "for clf, name in zip(classifier, classifier_name):    \n",
    "    #score = cross_val_score(clf, X_train, y_train)\n",
    "    score = cross_val_score(clf, features, labels, scoring='accuracy')\n",
    "    accuracy_model.append([name,score[0],score[1],score[2],score.mean()])\n",
    "  \n",
    "pd.DataFrame(accuracy_model,columns=('Model', \n",
    "                                     'Score1', \n",
    "                                     'Score2',\n",
    "                                     'Score3',\n",
    "                                     'Mean')).sort_values(by='Mean',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I put the Logistic Regression classifier even knowing it applys in for a continuous out which is not our case. As expected this classifier don't have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.97      0.94        38\n",
      "        1.0       0.50      0.20      0.29         5\n",
      "\n",
      "avg / total       0.86      0.88      0.86        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#FeatureUnion([(\"pca\", pca),\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "selection = SelectKBest(k=10)\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca),\n",
    "                                  (\"univ_select\",selection)])\n",
    "\n",
    "pipeline =   Pipeline([('features', combined_features),   \n",
    "                       ('rfr', RandomForestClassifier())\n",
    "                 ])\n",
    "\"\"\"\n",
    "pipeline =   Pipeline([('features', SelectKBest()),   \n",
    "                       ('rfr', RandomForestClassifier())\n",
    "                 ])\n",
    "\"\"\"\n",
    "pipeline.fit_transform(features_train, labels_train)\n",
    "pred = pipeline.predict(features_test)\n",
    "print 'Accuracy:', accuracy_score(labels_test, pred)\n",
    "print 'Recall:', recall_score(labels_test, pred)\n",
    "print 'Precision:', precision_score(labels_test, pred)\n",
    "#test_classifier(grid.best_estimator_, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "\n",
    "selection = SelectKBest(k=10)\n",
    "\n",
    "\"\"\"\n",
    "combined_features = FeatureUnion([(\"pca\", pca),\n",
    "                                  (\"univ_select\",selection)])\n",
    "\n",
    "pipeline =   Pipeline([('features', combined_features),   \n",
    "                       ('classify', RandomForestClassifier())\n",
    "                 ])\n",
    "\"\"\"\n",
    "pipeline =   Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                       ('features', selection),   \n",
    "                       ('classify', RandomForestClassifier())\n",
    "                      ])\n",
    "\n",
    "param_grid = {'scale': [None, MaxAbsScaler()],\n",
    "              \"classify__max_depth\": [5, 3, 1],\n",
    "              \"classify__max_features\": [2,1],\n",
    "              \"classify__min_samples_leaf\": [1, 3, 10,15],\n",
    "              \"classify__bootstrap\": [True, False],\n",
    "              \"classify__criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_\n",
    "clf =  grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [6] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('features', SelectKBest(k=10, score_func=<function f_classif at 0x000000000C1F3668>)), ('classify', RandomForestClassifier(bootstrap=False, class_weight=None,\n",
      "            criterion='entropy', max_depth=5, max_features=2,\n",
      "  ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.85227\tPrecision: 0.34164\tRecall: 0.11650\tF1: 0.17375\tF2: 0.13419\n",
      "\tTotal predictions: 15000\tTrue positives:  233\tFalse positives:  449\tFalse negatives: 1767\tTrue negatives: 12551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=10, score_func=<function f_classif at 0x000000000C23C668>)), ('classify', GradientBoostingClassifier(criterion='mae', init=None, learning_rate=1.0,\n",
      "              loss='deviance', max_depth=3, max_features=No...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.80667\tPrecision: 0.29282\tRecall: 0.31800\tF1: 0.30489\tF2: 0.31262\n",
      "\tTotal predictions: 15000\tTrue positives:  636\tFalse positives: 1536\tFalse negatives: 1364\tTrue negatives: 11464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                     ('selection', SelectKBest()),\n",
    "                     ('classify', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "        'scale': [MinMaxScaler(feature_range=(0, 1))],\n",
    "        'selection__k': [10],\n",
    "        'classify__criterion': ['mae', 'friedman_mse', 'mse'],\n",
    "        'classify__learning_rate':[1.0, 0.1],\n",
    "        'classify__min_samples_leaf': [3],\n",
    "        'classify__max_leaf_nodes': [100]\n",
    "        #'classify__loss' : ['deviance']\n",
    "    \n",
    "        #'classify__max_leaf_nodes': [100, 150],\n",
    "        #'classify__max_features': ['sqrt', 0.50, 0.80] \n",
    "        #'classify__subsample': [0.8, 1.0]\n",
    "        #'classify__loss' : ['exponential']\n",
    "        \n",
    "    }\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "predicted = grid.predict(features_test)\n",
    "test_classifier(grid.best_estimator_, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "           weights='distance'))])\n",
      "\tAccuracy: 0.84100\tPrecision: 0.31220\tRecall: 0.16000\tF1: 0.21157\tF2: 0.17729\n",
      "\tTotal predictions: 15000\tTrue positives:  320\tFalse positives:  705\tFalse negatives: 1680\tTrue negatives: 12295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', KNeighborsClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, StandardScaler(with_mean=False)],\n",
    "              #'selection__k': [6],\n",
    "              #'reduce_dim__n_components': [6],\n",
    "              #'classifier__metric': [\"euclidean\", \"cityblock\", 'minkowski'],\n",
    "              'classifier__n_neighbors' : [4,6,10,14],\n",
    "              'classifier__weights' : ['uniform','distance']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C695668>)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.80133\tPrecision: 0.35250\tRecall: 0.58550\tF1: 0.44006\tF2: 0.51713\n",
      "\tTotal predictions: 15000\tTrue positives: 1171\tFalse positives: 2151\tFalse negatives:  829\tTrue negatives: 10849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler()),\n",
    "                           ('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('selection', SelectKBest()),\n",
    "                           ('classifier', NearestCentroid())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, StandardScaler()],\n",
    "              'selection__k': [6],\n",
    "              'reduce_dim__n_components': [6],\n",
    "              'classifier__metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan','correlation', 'minkowski'],\n",
    "              'classifier__shrink_threshold'  : [None, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,1000.0]\n",
    "         }\n",
    "#scv = StratifiedShuffleSplit(labels_train, 1000, random_state = 42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=2, max_leaf_nodes=100, min_impurity_spl...one,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.82233\tPrecision: 0.32509\tRecall: 0.30900\tF1: 0.31684\tF2: 0.31209\n",
      "\tTotal predictions: 15000\tTrue positives:  618\tFalse positives: 1283\tFalse negatives: 1382\tTrue negatives: 11717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           #('reduce',PCA(n_components=6, random_state=42)),\n",
    "                           ('classifier', AdaBoostClassifier(random_state=42))\n",
    "                               ])\n",
    "params = {\n",
    "          #'selection__k': [6],\n",
    "          'classifier__base_estimator' : [DecisionTreeClassifier(max_features=2, criterion=\"gini\", max_leaf_nodes=100)], \n",
    "          'classifier__n_estimators': [150,200],\n",
    "          'classifier__learning_rate' :[0.1, 1.0],\n",
    "          'classifier__algorithm' : ['SAMME.R', 'SAMME']\n",
    "               }\n",
    "\n",
    "#scv = StratifiedShuffleSplit(features_train, 1000, random_state = 42)\n",
    "\n",
    "# set up gridsearch\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'accuracy', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b0c50803f744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\lieby\\Documents\\tester.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(clf, dataset, feature_list, folds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;31m### fit the classifier using training set, and test on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[0;32m--> 314\u001b[0;31m                                             random_state=random_state)\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\base.pyc\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[1;32m    121\u001b[0m                                     for p in self.estimator_params))\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m                             % (repr(estimator), type(estimator)))\n\u001b[1;32m     66\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[1;31m# This is set in utils/__init__.py but it gets overwritten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[1;31m# when running under python3 somehow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"always\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\warnings.pyc\u001b[0m in \u001b[0;36msimplefilter\u001b[0;34m(action, category, lineno, append)\u001b[0m\n\u001b[1;32m    106\u001b[0m     assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n\u001b[1;32m    107\u001b[0m                       \"once\"), \"invalid action: %r\" % (action,)\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlineno\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m            \u001b[1;34m\"lineno must be an int >= 0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('selection', SelectKBest()),\n",
    "                           ('classifier', RandomForestClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, MinMaxScaler(feature_range=(0, 1))],\n",
    "              'selection__k': [6, 10, 'all'],\n",
    "              #'reduce_dim__n_components': [6],\n",
    "              'classifier__n_estimators': [100, 150, 200],\n",
    "              'classifier__criterion'          : [\"gini\", \"entropy\"],\n",
    "              'classifier__max_depth'          : [5,10],\n",
    "              'classifier__min_samples_split'  : [3,2] ,\n",
    "              'classifier__bootstrap'          : [True, False]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=500, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.76653\tPrecision: 0.30195\tRecall: 0.57250\tF1: 0.39537\tF2: 0.48550\n",
      "\tTotal predictions: 15000\tTrue positives: 1145\tFalse positives: 2647\tFalse negatives:  855\tTrue negatives: 10353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', LogisticRegression(random_state=42))\n",
    "                          ])\n",
    "\n",
    "param_grid = {#'minmaxer' : [StandardScaler(with_mean=False), None],\n",
    "          #'selection__k': [10, 'all'],\n",
    "          'classifier__C': [0.05, 0.5, 1, 10, 100, 500, 1000],\n",
    "          'classifier__solver': ['liblinear'],\n",
    "          'classifier__penalty': ['l2'], \n",
    "          'classifier__class_weight': ['balanced']\n",
    "         }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85053\tPrecision: 0.42234\tRecall: 0.32900\tF1: 0.36987\tF2: 0.34421\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  900\tFalse negatives: 1342\tTrue negatives: 12100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
    "          n_estimators=50, random_state=None), my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85060\tPrecision: 0.42261\tRecall: 0.32900\tF1: 0.36997\tF2: 0.34425\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  899\tFalse negatives: 1342\tTrue negatives: 12101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(daBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
    "          n_estimators=50, random_state=None), my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tunning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
