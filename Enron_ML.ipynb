{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit\n",
    "    \n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import  MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression,RANSACRegressor, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Feature selection\n",
    "from sklearn.decomposition import PCA,RandomizedPCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest,chi2, f_regression\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,classification_report\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_label = 'poi'                \n",
    "email_features_list = [\n",
    "    # 'email_address', # remit email address; informational label\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    ]\n",
    "\n",
    "financial_features_list = [\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary'\n",
    "    #,\n",
    "    #'total_payments',\n",
    "    #'total_stock_value',\n",
    "]\n",
    "features_list = [target_label] + financial_features_list  \n",
    "#features_list = [target_label] + financial_features_list  + email_features_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"P:/Nanodegree/ML/ud120-projects/tools/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = [\"TOTAL\", \"THE TRAVEL AGENCY IN THE PARK\", \"LOCKHART EUGENE E\", \"CHAN RONNIE\"]\n",
    "for outlier in outliers :\n",
    "    data_dict.pop(outlier, 0)\n",
    "\n",
    "\n",
    "def update_dict_value(key, items, values, dict_obj):\n",
    "    index = 0\n",
    "    for item in items:     \n",
    "        dict_obj[key][item] = values[index]\n",
    "        index += 1\n",
    "    return dict_obj\n",
    "        \n",
    "\n",
    "    \n",
    "data_dict = update_dict_value(\n",
    "              'BELFER ROBERT',\n",
    "              ['deferred_income','deferral_payments', 'expenses', \n",
    "               'director_fees', 'total_payments', 'exercised_stock_options',\n",
    "               'restricted_stock','restricted_stock_deferred',\n",
    "               'total_stock_value'], \n",
    "              [-102500,'NaN',3285,102500, 3285,'NaN', 44093,-44093,'NaN'],\n",
    "              data_dict)\n",
    "\n",
    "\n",
    "data_dict = update_dict_value(\n",
    "              'BHATNAGAR SANJAY',\n",
    "              ['other', 'expenses', 'director_fees', 'total_payments',\n",
    "               'exercised_stock_options','restricted_stock',\n",
    "               'restricted_stock_deferred','total_stock_value'],\n",
    "              ['NaN',137864, 'NaN', 137864, 15456290, \n",
    "               2604490, -2604490, 15456290],\n",
    "               data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    key_values = data_dict[key]\n",
    "\n",
    "    total_msg = (data_dict[key]['to_messages'] + \n",
    "                 data_dict[key]['from_messages'])\n",
    "    \n",
    "    total_poi_msg = (data_dict[key]['from_poi_to_this_person'] +\n",
    "                     data_dict[key]['from_this_person_to_poi'] + \n",
    "                     data_dict[key]['shared_receipt_with_poi'])     \n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_poi_ratio'] = (float(total_poi_msg) / \n",
    "                                           float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_poi_ratio'] = \"NaN\"\n",
    "        \n",
    "    try:\n",
    "        data_dict[key]['message_others_ratio'] = ((float(total_msg) - float(total_poi_msg)) / \n",
    "                                          float(total_msg))\n",
    "    except:\n",
    "        data_dict[key]['message_others_ratio'] = \"NaN\"\n",
    "\n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features_list = features_list + email_features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "features_list = features_list + ['message_poi_ratio','message_others_ratio'] \n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# transformed version of X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, \n",
    "                                                                                            labels,  \n",
    "                                                                                            test_size=0.3, \n",
    "                                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tje features have quite differente value ranges and some of them are discrete and some of them take continous values, I need to scale them first. Removing mean and dividing the standard deviation o features respectively, this one of the most commonly used preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For now I don't know exactly the number of features I want to use, so I decided to use PCA for identify the most important features and explain the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.861553776249\n",
      "2 : 0.950132015173\n",
      "3 : 0.972382607444\n",
      "4 : 0.984291217765\n",
      "5 : 0.992901638092\n",
      "6 : 0.997163221659\n",
      "7 : 0.998629887029\n",
      "8 : 0.999743067191\n",
      "9 : 0.99987985018\n",
      "10 : 0.999974804369\n",
      "11 : 0.999991386498\n",
      "12 : 1.0\n",
      "13 : 1.0\n",
      "14 : 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    pca = PCA(svd_solver='auto', n_components=i)\n",
    "    x = pca.fit(features_train).explained_variance_ratio_.sum()\n",
    "    print i,\":\", x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this decomposition, the vector array provided by ratio indicates that most of the information is concentrated into the first 2 compontents . You saw this same sort of result after the factor analysis. it's therefore possible to reduce the entire dataset to just two componentes, providing a reduction of noise and redundant information from the original dataset.\n",
    "\n",
    "Let's see which variables are the most important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>variance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bonus</td>\n",
       "      <td>41.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>22.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>12.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>7.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>5.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expenses</td>\n",
       "      <td>4.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>2.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>message_poi_ratio</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>message_others_ratio</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  variance_ratio\n",
       "0                       bonus          41.347\n",
       "1           deferral_payments          22.313\n",
       "2             deferred_income          12.831\n",
       "3               director_fees           7.052\n",
       "4     exercised_stock_options           5.041\n",
       "5                    expenses           4.009\n",
       "6               loan_advances           2.084\n",
       "7         long_term_incentive           1.738\n",
       "8                       other           1.270\n",
       "9            restricted_stock           0.985\n",
       "10  restricted_stock_deferred           0.528\n",
       "11                     salary           0.450\n",
       "12          message_poi_ratio           0.207\n",
       "13       message_others_ratio           0.143"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=14)\n",
    "pca.fit_transform(X_train)\n",
    "pca_df = pd.DataFrame(zip(features_list[1:],\n",
    "                          np.round(pca.explained_variance_ratio_, decimals=5)*100), \n",
    "                      columns=['feature','variance_ratio'])\n",
    "pca_df.sort_values(by='variance_ratio',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't wanna that the features negatively impact my model performance. I'll spend more time on this and try SelecKbest for in this task. \n",
    "\n",
    "First of all, I scaled the data with MinMaxScaler (range 0, 1), then I applyied SelectKBest chi squared statistical test, cause this test need non-negative features. After that I used SelectKBest ANOVA F-value statistical test on the raw data, in a way I could compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest: \n",
      "                      feature      anova      chi2\n",
      "4     exercised_stock_options  22.087532  6.246159\n",
      "0                       bonus  20.524645  5.048256\n",
      "11                     salary  18.003740  2.989183\n",
      "2             deferred_income  11.320185  0.338413\n",
      "12          message_poi_ratio   9.816852  3.371674\n",
      "14          message_poi_ratio   9.816852  3.371674\n",
      "7         long_term_incentive   9.772104  2.497366\n",
      "9            restricted_stock   8.694888  2.463442\n",
      "6               loan_advances   7.125382  6.634816\n",
      "5                    expenses   5.287549  1.293600\n",
      "8                       other   4.143788  1.703679\n",
      "3               director_fees   1.972788  1.508957\n",
      "13       message_others_ratio   1.473822  0.392669\n",
      "15       message_others_ratio   1.473822  0.392669\n",
      "10  restricted_stock_deferred   0.767702  0.008756\n",
      "1           deferral_payments   0.236711  0.094344\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "selector = SelectKBest(chi2, k='all').fit(features_scaled, labels)\n",
    "k_best = SelectKBest(f_classif,k='all').fit(features, labels)    \n",
    "\n",
    "# Format values\n",
    "kbest_pd = pd.DataFrame(zip(features_list[1:],\n",
    "                            k_best.scores_, # K best score                            \n",
    "                            selector.scores_), # chi2\n",
    "                        columns = ['feature','anova', 'chi2'])\n",
    "print \"SelectKBest: \"\n",
    "print kbest_pd.sort_values(by='anova',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing classifier: Naive Bayes\n",
      "Start testing classifier: DecisionTree\n",
      "Start testing classifier: RandomForest\n",
      "Start testing classifier: GradientBoostingClassifier\n",
      "Start testing classifier: AdaBoost\n",
      "Start testing classifier: ExtraTreesClassifier\n",
      "Start testing classifier: LogisticRegression\n",
      "Start testing classifier: KNeighbors\n",
      "Start testing classifier: NearestCentroid\n",
      "                        Model  Precision  Recall  Accuracy\n",
      "7                  KNeighbors   0.705556  0.1270  0.876533\n",
      "2                RandomForest   0.401084  0.1480  0.856933\n",
      "4                    AdaBoost   0.418319  0.3060  0.850733\n",
      "5        ExtraTreesClassifier   0.362011  0.1620  0.850200\n",
      "8             NearestCentroid   0.376596  0.2655  0.843467\n",
      "3  GradientBoostingClassifier   0.263037  0.2295  0.811533\n",
      "1                DecisionTree   0.251683  0.2430  0.802733\n",
      "6          LogisticRegression   0.164218  0.1900  0.763067\n",
      "0                 Naive Bayes   0.219135  0.6230  0.653733\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "# naive_bayes\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "# tree\n",
    "models.append(('DecisionTree', DecisionTreeClassifier(random_state=42)))\n",
    "# ensemble\n",
    "models.append(('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0)))\n",
    "\n",
    "models.append(('AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=42)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=100, random_state=42)))\n",
    "# linear_model\n",
    "models.append(('LogisticRegression', LogisticRegression(random_state=42)))\n",
    "# neighbors\n",
    "models.append(('KNeighbors', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('NearestCentroid', NearestCentroid()))\n",
    "\n",
    "accuracy_model = []\n",
    "score = []\n",
    "for name, clf in models:  \n",
    "    tm = time()\n",
    "    #print \"Start testing classifier:\", name\n",
    "\n",
    "    score = test_classifier(clf, my_dataset, features_list)\n",
    "    #print score\n",
    "    accuracy_model.append([name,score[2], score[3], score[1]])\n",
    "    #print \"Successfully completed classifier test.\"\n",
    "    #print \"Processing time: {0}\".format(round(time()-tm, 3))\n",
    "\n",
    "scores = pd.DataFrame(accuracy_model,\n",
    "                      columns=('Model', \n",
    "                               'Precision', \n",
    "                               'Recall',\n",
    "                               'Accuracy')).sort_values(by='Accuracy',\n",
    "                                                    ascending = False)\n",
    "\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm interested in the three metrics recall, precision and accuracy in the test with basic parameters, the Models NearestCentroid and GradientBoostingClassifier had a score close to the expected of 0.3. Even without tuning the AdaBoost reached the metric with a precision of 0.41, recall of 0.30 and a good percentage of acuracia of 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the models has reached the goal, but what if we can increase the precision and recall of the other models? Sounds like a good mission, does not it? We, will try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C0835F8>)), ('classify', GradientBoostingClassifier(criterion='mae', init=None, learning_rate=0.1,\n",
      "              loss='deviance', max_depth=3, max_features=Non...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.84187\tPrecision: 0.33029\tRecall: 0.18100\tF1: 0.23385\tF2: 0.19899\n",
      "\tTotal predictions: 15000\tTrue positives:  362\tFalse positives:  734\tFalse negatives: 1638\tTrue negatives: 12266\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C0835F8>)), ('classify', GradientBoostingClassifier(criterion='mae', init=None, learning_rate=0.1,\n",
       "               loss='deviance', max_depth=3, max_features=Non...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "               warm_start=False))]),\n",
       " 0.8418666666666667,\n",
       " 0.3302919708029197,\n",
       " 0.181,\n",
       " 0.23385012919896642,\n",
       " 0.1989885664028144)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                     ('selection', SelectKBest()),\n",
    "                     ('classify', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "        'selection__k': [6],\n",
    "        'classify__criterion': ['mae', 'friedman_mse', 'mse'],\n",
    "        'classify__learning_rate':[1.0, 0.1],\n",
    "        'classify__min_samples_leaf': [3],\n",
    "        'classify__max_leaf_nodes': [100]\n",
    "        #'classify__loss' : ['deviance']\n",
    "    \n",
    "        #'classify__max_leaf_nodes': [100, 150],\n",
    "        #'classify__max_features': ['sqrt', 0.50, 0.80] \n",
    "        #'classify__subsample': [0.8, 1.0]\n",
    "        #'classify__loss' : ['exponential']\n",
    "        \n",
    "    }\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "predicted = grid.predict(features_test)\n",
    "test_classifier(grid.best_estimator_, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C0835F8>)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.80133\tPrecision: 0.35250\tRecall: 0.58550\tF1: 0.44006\tF2: 0.51713\n",
      "\tTotal predictions: 15000\tTrue positives: 1171\tFalse positives: 2151\tFalse negatives:  829\tTrue negatives: 10849\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
       "   svd_solver='auto', tol=0.0, whiten=False)), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C0835F8>)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=None))]),\n",
       " 0.8013333333333333,\n",
       " 0.3524984948826008,\n",
       " 0.5855,\n",
       " 0.4400601277715145,\n",
       " 0.5171347818406643)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler()),\n",
    "                           ('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('selection', SelectKBest()),\n",
    "                           ('classifier', NearestCentroid())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, StandardScaler()],\n",
    "              'selection__k': [6],\n",
    "              'reduce_dim__n_components': [6],\n",
    "              'classifier__metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan','correlation', 'minkowski'],\n",
    "              'classifier__shrink_threshold'  : [None, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,1000.0]\n",
    "         }\n",
    "#scv = StratifiedShuffleSplit(labels_train, 1000, random_state = 42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Last but not least...  AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=2, max_leaf_nodes=100, min_impurity_spl...one,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.82233\tPrecision: 0.32509\tRecall: 0.30900\tF1: 0.31684\tF2: 0.31209\n",
      "\tTotal predictions: 15000\tTrue positives:  618\tFalse positives: 1283\tFalse negatives: 1382\tTrue negatives: 11717\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=2, max_leaf_nodes=100, min_impurity_spl...one,\n",
       "             splitter='best'),\n",
       "           learning_rate=0.1, n_estimators=150, random_state=42))]),\n",
       " 0.8223333333333334,\n",
       " 0.3250920568122041,\n",
       " 0.309,\n",
       " 0.31684183542681366,\n",
       " 0.3120896879103121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           #('reduce',PCA(n_components=6, random_state=42)),\n",
    "                           ('classifier', AdaBoostClassifier(random_state=42))\n",
    "                               ])\n",
    "params = {\n",
    "          #'selection__k': [6],\n",
    "          'classifier__base_estimator' : [DecisionTreeClassifier(max_features=2, criterion=\"gini\", max_leaf_nodes=100)], \n",
    "          'classifier__n_estimators': [150,200],\n",
    "          'classifier__learning_rate' :[0.1, 1.0],\n",
    "          'classifier__algorithm' : ['SAMME.R', 'SAMME']\n",
    "               }\n",
    "\n",
    "#scv = StratifiedShuffleSplit(features_train, 1000, random_state = 42)\n",
    "\n",
    "# set up gridsearch\n",
    "grid = GridSearchCV(pipeline, param_grid = params,scoring = 'accuracy', cv=10)\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now I'm a bit curious...\n",
    "about how much other models can be improved. Let's continue the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the tuning task, I preset 3 estimator with a list of transformer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale_pca_kbest = FeatureUnion([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                                (\"pca\", PCA()),\n",
    "                                (\"selection\",SelectKBest())\n",
    "                               ])\n",
    "\n",
    "scale_kbest = FeatureUnion([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                            (\"selection\",SelectKBest())\n",
    "                           ])\n",
    "\n",
    "scale_pca = FeatureUnion([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                          (\"pca\", PCA())\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [6] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('scale', None), ('select', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C0835F8>)), ('classify', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=5, max_features=2, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07,...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False))]),\n",
       " 0.8510666666666666,\n",
       " 0.30236486486486486,\n",
       " 0.0895,\n",
       " 0.1381172839506173,\n",
       " 0.10416666666666667)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline =   Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                       ('select', SelectKBest()),   \n",
    "                       ('classify', RandomForestClassifier())\n",
    "                      ])\n",
    "\n",
    "param_grid = {'scale': [None, MaxAbsScaler()],\n",
    "              'select__k': [6, 10],\n",
    "              \"classify__max_depth\": [5, 3, 1],\n",
    "              \"classify__max_features\": [2,1],\n",
    "              \"classify__min_samples_leaf\": [1, 3, 10,15],\n",
    "              \"classify__bootstrap\": [True, False],\n",
    "              \"classify__criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='recall')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf =  grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "           weights='distance'))])\n",
      "\tAccuracy: 0.84100\tPrecision: 0.31220\tRecall: 0.16000\tF1: 0.21157\tF2: 0.17729\n",
      "\tTotal predictions: 15000\tTrue positives:  320\tFalse positives:  705\tFalse negatives: 1680\tTrue negatives: 12295\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "            weights='distance'))]),\n",
       " 0.841,\n",
       " 0.3121951219512195,\n",
       " 0.16,\n",
       " 0.2115702479338843,\n",
       " 0.17728531855955681)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', KNeighborsClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, StandardScaler(with_mean=False)],\n",
    "              #'selection__k': [6],\n",
    "              #'reduce_dim__n_components': [6],\n",
    "              #'classifier__metric': [\"euclidean\", \"cityblock\", 'minkowski'],\n",
    "              'classifier__n_neighbors' : [4,6,10,14],\n",
    "              'classifier__weights' : ['uniform','distance']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, cv=10, scoring='f1')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('selection', SelectKBest(k=6, score_func=<function f_classif at 0x000000000C695668>)), ('classifier', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.80133\tPrecision: 0.35250\tRecall: 0.58550\tF1: 0.44006\tF2: 0.51713\n",
      "\tTotal predictions: 15000\tTrue positives: 1171\tFalse positives: 2151\tFalse negatives:  829\tTrue negatives: 10849\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', StandardScaler(copy=True, with_mean=False, with_std=True)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=2, max_leaf_nodes=100, min_impurity_spl...one,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=150, random_state=42))])\n",
      "\tAccuracy: 0.82233\tPrecision: 0.32509\tRecall: 0.30900\tF1: 0.31684\tF2: 0.31209\n",
      "\tTotal predictions: 15000\tTrue positives:  618\tFalse positives: 1283\tFalse negatives: 1382\tTrue negatives: 11717\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b0c50803f744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\lieby\\Documents\\tester.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(clf, dataset, feature_list, folds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;31m### fit the classifier using training set, and test on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[0;32m--> 314\u001b[0;31m                                             random_state=random_state)\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\base.pyc\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[1;32m    121\u001b[0m                                     for p in self.estimator_params))\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m                             % (repr(estimator), type(estimator)))\n\u001b[1;32m     66\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[1;31m# This is set in utils/__init__.py but it gets overwritten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[1;31m# when running under python3 somehow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"always\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lieby\\Anaconda2\\lib\\warnings.pyc\u001b[0m in \u001b[0;36msimplefilter\u001b[0;34m(action, category, lineno, append)\u001b[0m\n\u001b[1;32m    106\u001b[0m     assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n\u001b[1;32m    107\u001b[0m                       \"once\"), \"invalid action: %r\" % (action,)\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlineno\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m            \u001b[1;34m\"lineno must be an int >= 0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           ('selection', SelectKBest()),\n",
    "                           ('classifier', RandomForestClassifier())\n",
    "                          ])\n",
    "\n",
    "param_grid = {'minmaxer' : [None, MinMaxScaler(feature_range=(0, 1))],\n",
    "              'selection__k': [6, 10, 'all'],\n",
    "              #'reduce_dim__n_components': [6],\n",
    "              'classifier__n_estimators': [100, 150, 200],\n",
    "              'classifier__criterion'          : [\"gini\", \"entropy\"],\n",
    "              'classifier__max_depth'          : [5,10],\n",
    "              'classifier__min_samples_split'  : [3,2] ,\n",
    "              'classifier__bootstrap'          : [True, False]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=500, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.76653\tPrecision: 0.30195\tRecall: 0.57250\tF1: 0.39537\tF2: 0.48550\n",
      "\tTotal predictions: 15000\tTrue positives: 1145\tFalse positives: 2647\tFalse negatives:  855\tTrue negatives: 10353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=500, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       " 0.7665333333333333,\n",
       " 0.30195147679324896,\n",
       " 0.5725,\n",
       " 0.3953729281767956,\n",
       " 0.48549864314789687)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('minmaxer', MinMaxScaler(feature_range=(0, 1))),\n",
    "                           #('minmaxer', StandardScaler(with_mean=False)),\n",
    "                           #('reduce_dim', PCA(copy=True, random_state=42)),\n",
    "                           #('selection', SelectKBest()),\n",
    "                           ('classifier', LogisticRegression(random_state=42))\n",
    "                          ])\n",
    "\n",
    "param_grid = {#'minmaxer' : [StandardScaler(with_mean=False), None],\n",
    "          #'selection__k': [10, 'all'],\n",
    "          'classifier__C': [0.05, 0.5, 1, 10, 100, 500, 1000],\n",
    "          'classifier__solver': ['liblinear'],\n",
    "          'classifier__penalty': ['l2'], \n",
    "          'classifier__class_weight': ['balanced']\n",
    "         }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(features_train, labels_train)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85053\tPrecision: 0.42234\tRecall: 0.32900\tF1: 0.36987\tF2: 0.34421\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  900\tFalse negatives: 1342\tTrue negatives: 12100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
    "          n_estimators=50, random_state=None), my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85060\tPrecision: 0.42261\tRecall: 0.32900\tF1: 0.36997\tF2: 0.34425\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  899\tFalse negatives: 1342\tTrue negatives: 12101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(daBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
    "          n_estimators=50, random_state=None), my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tunning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
